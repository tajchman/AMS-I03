<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript" src="http://ccrs.cac.cornell.edu:8080//ace/ace.js" charset="utf-8"></script>
<script type="application/javascript" src="http://ccrs.cac.cornell.edu:8080//target/web-client-jsdeps.js"></script>
<!-- include application -->
<script type="application/javascript" src="http://ccrs.cac.cornell.edu:8080//target/web-client-opt.js"></script>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>Hybrid computing</h1>
        <h5>Experimental html version of downloadable textbook, see http://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>


\[
\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\]


40.1 : <a href="hybrid.html#Discussion">Discussion</a><br>
40.2 : <a href="hybrid.html#HybridMPI-plus-threadsexecution">Hybrid MPI-plus-threads execution</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>40 Hybrid computing</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

So far, you have learned to use MPI for distributed memory and OpenMP
for shared memory parallel programming. However, distribute memory
architectures actually have a shared memory component, since each
cluster node is typically of a multicore design. Accordingly, you
could program your cluster using MPI for inter-node and OpenMP for
intra-node parallelism.
</p>

<p name="switchToTextMode">
Say you use 100 cluster nodes, each with 16 cores. You could then
start 1600 MPI processes, one for each core, but you could also start
100 processes, and give each access to 16 OpenMP threads.
</p>

<!-- environment: tacc start embedded generator -->
<!-- environment block purpose: [[ environment=tacc ]] -->
<tacc>

<p name="tacc">
<!-- TranslatingLineGenerator tacc ['tacc'] -->
In your slurm scripts, the first scenario would be specified \n{-N 100
-n 1600}, and the second as
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
#$ SBATCH -N 100
#$ SBATCH -n 100


export OMP_NUM_THREADS=16
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->

</tacc>
<!-- environment: tacc end embedded generator -->
<p name="switchToTextMode">

There is a third choice, in between these extremes, that makes
sense. A&nbsp;cluster node often has more than one socket, so you could put
one MPI process on each 
<i>socket</i>
, and use a number of
threads equal to the number of cores per socket.
</p>

<!-- environment: tacc start embedded generator -->
<!-- environment block purpose: [[ environment=tacc ]] -->
<tacc>

<p name="tacc">
<!-- TranslatingLineGenerator tacc ['tacc'] -->
The script for this would be:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
#$ SBATCH -N 100
#$ SBATCH -n 200


export OMP_NUM_THREADS=8
ibrun tacc_affinity yourprogram
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

The 
<tt>tacc_affinity</tt>
 script unsets the following variables:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
export MV2_USE_AFFINITY=0
export MV2_ENABLE_AFFINITY=0
export VIADEV_USE_AFFINITY=0
export VIADEV_ENABLE_AFFINITY=0
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
If you don't use 
<tt>tacc_affinity</tt>
 you may want to do this by hand,
otherwise 
<i>mvapich2</i>
 will use its own affinity rules.
</p name="tacc">

</tacc>
<!-- environment: tacc end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/mpi-omp-hybrid.png" width=800>
<p name="caption">
FIGURE 40.1: Three modes of MPI/OpenMP usage on a multi-core cluster
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
40.1
 illustrates these three modes: pure MPI
with no threads used; one MPI process per node and full
multi-threading; two MPI processes per node, one per socket, and
multiple threads on each socket.
</p>

<h2><a id="Discussion">40.1</a> Discussion</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#Discussion">Discussion</a>
</p>
<p name="switchToTextMode">

The performance implications of the pure MPI strategy versus hybrid
are subtle.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
First of all, we note that there is no obvious speedup: in a
  well balanced MPI application all cores are busy all the time, so
  using threading can give no immediate improvement.
<li>
Both MPI and OpenMP are subject to Amdahl's law that quantifies
  the influence of sequential code; in hybrid computing there is a new
  version of this law regarding the amount of code that is
  MPI-parallel, but not OpenMP-parallel.
<li>
MPI processes run unsynchronized, so small variations in load or
  in processor behaviour can be tolerated. The frequent barriers in
  OpenMP constructs make a hybrid code more tightly synchronized, so
  load balancing becomes more critical.
<li>
On the other hand, in OpenMP codes it is easier to divide the
  work into more tasks than there are threads, so statistically a
  certain amount of load balancing happens automatically.
<li>
Each MPI process has its own buffers, so hybrid takes less
  buffer overhead.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Review the scalability argument for 1D versus 2D matrix
  decomposition in 
<i>Eijkhout:IntroHPC</i>
. Would you get
  scalable performance from doing a 1D decomposition (for instance, of
  the rows) over MPI processes, and decomposing the other directions
  (the columns) over OpenMP threads?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Another performance argument we need to consider concerns message
traffic.  If let all threads make MPI calls (see
section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/hybrid.html#HybridMPI-plus-threadsexecution">40.2</a>
) there is going to be little
difference. However, in one popular hybrid computing strategy we would
keep MPI calls out of the OpenMP regions and have them in effect done
by the master thread.
In that case there are only MPI messages
between nodes, instead of between cores. This leads to a decrease in
message traffic, though this is hard to quantify. The number of
messages goes down approximately by the number of cores per node, so
this is an advantage if the average message size is small. On the
other hand, the amount of data sent is only reduced if there is
overlap in content between the messages.
</p>

<p name="switchToTextMode">
Limiting MPI traffic to the master thread also means that no buffer
space is needed for the on-node communication.
</p>

<h2><a id="HybridMPI-plus-threadsexecution">40.2</a> Hybrid MPI-plus-threads execution</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#HybridMPI-plus-threadsexecution">Hybrid MPI-plus-threads execution</a>
</p>


<p name="switchToTextMode">

In hybrid execution, the main question is whether all threads
are allowed to make MPI calls. To determine this,
replace the 
<tt>MPI_Init</tt>
 call by
<i>MPI_Init_thread</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Init_thread" aria-expanded="false" aria-controls="MPI_Init_thread">
        Routine reference: MPI_Init_thread
      </button>
    </h5>
  </div>
  <div id="MPI_Init_thread" class="collapse">
  <pre>
C:
int MPI_Init_thread(int *argc, char ***argv, int required, int *provided)

Fortran:
MPI_Init_thread(required, provided, ierror)
INTEGER, INTENT(IN) :: required
INTEGER, INTENT(OUT) :: provided
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
</pre>
</div>
</div>
Here the 
<tt>required</tt>
 and 
<tt>provided</tt>
 parameters can take the following
(monotonically increasing) values:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
<i>MPI_THREAD_SINGLE</i>
: Only a single thread will
  execute.
<li>
<i>MPI_THREAD_FUNNELED</i>
: The program may use multiple
  threads, but only the main thread will make MPI calls.
</p>

<p name="switchToTextMode">
    The main thread is usually the one selected by the
<tt>master</tt>
 directive, but technically it is the only that
    executes 
<i>MPI_Init_thread</i>
. If you call this routine in
    a parallel region, the main thread may be different from the master.
<li>
<i>MPI_THREAD_SERIALIZED</i>
: The program may use multiple
  threads, all of which may make MPI calls, but there will never be
  simultaneous MPI calls in more than one thread.
<li>
<i>MPI_THREAD_MULTIPLE</i>
: Multiple threads may issue MPI
  calls, without restrictions.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

After the initialization call, you can query the support level
with 
<i>MPI_Query_thread</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Query_thread" aria-expanded="false" aria-controls="MPI_Query_thread">
        Routine reference: MPI_Query_thread
      </button>
    </h5>
  </div>
  <div id="MPI_Query_thread" class="collapse">
  <pre>
C:
int MPI_Query_thread(int *provided)

Fortran:
MPI_Query_thread(provided, ierror)
INTEGER, INTENT(OUT) :: provided
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
</pre>
</div>
</div>
.
</p>

<p name="switchToTextMode">
In case more than one thread performs communication,
<i>MPI_Is_thread_main</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Is_thread_main" aria-expanded="false" aria-controls="MPI_Is_thread_main">
        Routine reference: MPI_Is_thread_main
      </button>
    </h5>
  </div>
  <div id="MPI_Is_thread_main" class="collapse">
  <pre>
C:
int MPI_Is_thread_main(int *flag)

Fortran:
MPI_Is_thread_main(flag, ierror)
LOGICAL, INTENT(OUT) :: flag
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
</pre>
</div>
</div>
can determine whether a thread is the main thread.
</p>

<!-- environment: mplnote start embedded generator -->
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
enum mpl::threading_modes {
  mpl::threading_modes::single = MPI_THREAD_SINGLE,
  mpl::threading_modes::funneled = MPI_THREAD_FUNNELED,
  mpl::threading_modes::serialized = MPI_THREAD_SERIALIZED,
  mpl::threading_modes::multiple = MPI_THREAD_MULTIPLE
};
threading_modes mpl::environment::threading_mode ();
bool mpl::environment::is_thread_main ();
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: tacc start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=tacc ]] -->
<tacc>

<p name="tacc">
<!-- TranslatingLineGenerator tacc ['tacc'] -->
  The 
<i>mvapich</i>
 implementation of MPI
  does have the required threading support, but you need to set this environment variable:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
export MV2_ENABLE_AFFINITY=0
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  Another solution is to run your code like this:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
  ibrun tacc_affinity &lt;my_multithreaded_mpi_executable
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  Intel MPI uses an environment variable to turn on thread support:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
I_MPI_LIBRARY_KIND=&lt;value&gt;
where
release : multi-threaded with global lock
release_mt : multi-threaded with per-object lock for thread-split
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->

</tacc>
<!-- environment: tacc end embedded generator -->
<p name="switchToTextMode">

The 
<i>mpirun</i>

<!-- index -->
program usually propagates 
<i>environment variables</i>
,
so the value of 
<tt>OMP_NUM_THREADS</tt>
 when you call 
<tt>mpirun</tt>

will be seen by each MPI process.
</p>

<!-- environment: itemize start embedded generator -->
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
It is possible to use blocking sends in threads, and let the
  threads block. This does away with the need for polling.
<li>
You can not send to a thread number: use the MPI
<i>message tag</i>
 to send to a specific thread.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
Consider the 2D heat equation and explore the mix of MPI/OpenMP
parallelism:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Give each node one MPI process that is fully multi-threaded.
<li>
Give each core an MPI process and don't use multi-threading.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Discuss theoretically why the former can give higher performance.
Implement both schemes as special cases of the general hybrid case,
and run tests to find the optimal mix.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#thread" aria-expanded="false" aria-controls="thread">
        C Code: thread
      </button>
    </h5>
  </div>
  <div id="thread" class="collapse">
  <pre>
// thread.c
MPI_Init_thread(&argc,&argv,MPI_THREAD_MULTIPLE,&threading);
comm = MPI_COMM_WORLD;
MPI_Comm_rank(comm,&procno);
MPI_Comm_size(comm,&nprocs);

if (procno==0) {
  switch (threading) {
  case MPI_THREAD_MULTIPLE : printf("Glorious multithreaded MPI\n"); break;
  case MPI_THREAD_SERIALIZED : printf("No simultaneous MPI from threads\n"); break;
  case MPI_THREAD_FUNNELED : printf("MPI from main thread\n"); break;
  case MPI_THREAD_SINGLE : printf("no threading supported\n"); break;
  }
}
MPI_Finalize();
</pre>
</div>
</div>
</div>
<a href="index.html">Back to Table of Contents</a>
