<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript" src="http://ccrs.cac.cornell.edu:8080//ace/ace.js" charset="utf-8"></script>
<script type="application/javascript" src="http://ccrs.cac.cornell.edu:8080//target/web-client-jsdeps.js"></script>
<!-- include application -->
<script type="application/javascript" src="http://ccrs.cac.cornell.edu:8080//target/web-client-opt.js"></script>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>MPI topic: Shared memory</h1>
        <h5>Experimental html version of downloadable textbook, see http://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>


\[
\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\]


12.1 : <a href="mpi-shared.html#Recognizingsharedmemory">Recognizing shared memory</a><br>
12.2 : <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a><br>
12.2.1 : <a href="mpi-shared.html#Pointerstoasharedwindow">Pointers to a shared window</a><br>
12.2.2 : <a href="mpi-shared.html#Queryingthesharedstructure">Querying the shared structure</a><br>
12.2.3 : <a href="mpi-shared.html#Heatequationexample">Heat equation example</a><br>
12.2.4 : <a href="mpi-shared.html#Sharedbulkdata">Shared bulk data</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>12 MPI topic: Shared memory</h1>
<!-- TranslatingLineGenerator file ['file'] -->
</p>

<!-- index -->
<p name="switchToTextMode">

Some programmers are under the impression that MPI would not be efficient on
shared memory, since all operations are done through what looks like network calls.
This is not correct: many MPI
implementations have optimizations that detect shared memory and can
exploit it, so that data is copied, rather than going through a communication layer.
(Conversely, programming systems for shared memory such as 
<i>OpenMP</i>
can actually have inefficiencies associated with thread handling.)
The main inefficiency associated with using MPI on shared memory is then
that processes can not actually share data.
</p>

<p name="switchToTextMode">
The one-sided MPI calls (chapter&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-onesided.html">MPI topic: One-sided communication</a>
) can also be used to
emulate shared memory, in the sense that an origin process can access data
from a target process without the target's active involvement.
However, these calls do not distinguish between actually shared memory
and one-sided access across the network.
</p>

<p name="switchToTextMode">
In this chapter we will look at the ways MPI
can interact with the presence of actual shared memory.
(This functionality was added in the \mpistandard{3} standard.)
This relies on the 
<i>MPI_Win</i>
 windows concept, but
otherwise uses direct access of other processes' memory.
</p>

<h2><a id="Recognizingsharedmemory">12.1</a> Recognizing shared memory</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Recognizingsharedmemory">Recognizing shared memory</a>
</p>

<p name="switchToTextMode">

MPI's one-sided routines take a very symmetric view of processes:
each process can access the window of every other process (within a communicator).
Of course, in practice there will be a difference in performance
depending on whether the origin and target are actually
on the same shared memory, or whether they can only communicate through the network.
For this reason MPI makes it easy to group processes by shared memory domains
using 
<i>MPI_Comm_split_type</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Comm_split_type" aria-expanded="false" aria-controls="MPI_Comm_split_type">
        Routine reference: MPI_Comm_split_type
      </button>
    </h5>
  </div>
  <div id="MPI_Comm_split_type" class="collapse">
  <pre>
C:
int MPI_Comm_split_type(
  MPI_Comm comm, int split_type, int key,
  MPI_Info info, MPI_Comm *newcomm)

Fortran:
MPI_Comm_split_type(comm, split_type, key, info, newcomm, ierror)
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, INTENT(IN) :: split_type, key
TYPE(MPI_Info), INTENT(IN) :: info
TYPE(MPI_Comm), INTENT(OUT) :: newcomm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python:
MPI.Comm.Split_type(
  self, int split_type, int key=0, Info info=INFO_NULL)
</pre>
</div>
</div>
.
</p>

<p name="switchToTextMode">
Here the 
<tt>split_type</tt>
 parameter has to be from the following (short) list:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
<i>MPI_COMM_TYPE_SHARED</i>
: split the communicator into subcommunicators
  of processes sharing a memory area.
<!-- environment: mpifour start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mpifour ]] -->
<remark>
<b>MPI 4 Standard only</b>
<p name="remark">
<!-- TranslatingLineGenerator mpifour ['mpifour'] -->
<li>
<i>MPI_COMM_TYPE_HW_GUIDED</i>
 (\mpistandard{4}):
    split using an 
<tt>info</tt>
 value from 
<i>MPI_Get_hw_resource_types</i>
.
<li>
<i>MPI_COMM_TYPE_HW_UNGUIDED</i>
 (\mpistandard{4}):
    similar to 
<i>MPI_COMM_TYPE_HW_GUIDED</i>
, but the resulting communicators
    should be a strict subset of the original communicator.
    On processes where this condition can not be fullfilled,
<i>MPI_COMM_NULL</i>
 will be returned.
</p name="remark">
<i>End of MPI 4 note</i>
</remark>
<!-- environment: mpifour end embedded generator -->
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  Similar to ordinary communicator splitting&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-comm.html#Splittingacommunicator">7.4</a>
:
   <tt>communicator::</tt> 
<i>split_shared</i>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

In the following example, 
<tt>CORES_PER_NODE</tt>
 is a platform-dependent
constant:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#commsplittype" aria-expanded="false" aria-controls="commsplittype">
        C Code: commsplittype
      </button>
    </h5>
  </div>
  <div id="commsplittype" class="collapse">
  <pre>
// commsplittype.c
MPI_Info info;
MPI_Comm_split_type(MPI_COMM_WORLD,MPI_COMM_TYPE_SHARED,procno,info,&sharedcomm);
MPI_Comm_size(sharedcomm,&new_nprocs);
MPI_Comm_rank(sharedcomm,&new_procno);
</pre>
</div>
</div>
</p>

<h2><a id="Sharedmemoryforwindows">12.2</a> Shared memory for windows</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a>
</p>
<p name="switchToTextMode">

Processes that exist on the same physical shared memory should be able
to move data by copying, rather than through MPI send/receive calls
--&nbsp;which of course will do a copy operation under the hood.
In order to do such user-level copying:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
We need to create a shared memory area with
<i>MPI_Win_allocate_shared</i>
, and
<li>
We need to get pointers to where a process' area is in this
  shared space; this is done with 
<i>MPI_Win_shared_query</i>
.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Pointerstoasharedwindow">12.2.1</a> Pointers to a shared window</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Pointerstoasharedwindow">Pointers to a shared window</a>
</p>
</p>

<p name="switchToTextMode">
The first step is to create a window (in the sense of one-sided MPI;
section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-onesided.html#Windows">9.1</a>
) on the processes on one node.
Using the 
<i>MPI_Win_allocate_shared</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Win_allocate_shared" aria-expanded="false" aria-controls="MPI_Win_allocate_shared">
        Routine reference: MPI_Win_allocate_shared
      </button>
    </h5>
  </div>
  <div id="MPI_Win_allocate_shared" class="collapse">
  <pre>
Semantics:
MPI_WIN_ALLOCATE_SHARED(size, disp_unit, info, comm, baseptr, win)

Input parameters:
size: size of local window in bytes (non-negative integer)
disp_unit local unit size for displacements, in bytes (positive
integer)
info: info argument (handle)
comm: intra-communicator (handle)

Output parameters:
baseptr: address of local allocated window segment (choice)
win: window object returned by the call (handle)

C:
int MPI_Win_allocate_shared
   (MPI_Aint size, int disp_unit, MPI_Info info,
    MPI_Comm comm, void *baseptr, MPI_Win *win)

Fortran:
MPI_Win_allocate_shared
   (size, disp_unit, info, comm, baseptr, win, ierror)
USE, INTRINSIC :: ISO_C_BINDING, ONLY : C_PTR
INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(IN) :: size
INTEGER, INTENT(IN) :: disp_unit
TYPE(MPI_Info), INTENT(IN) :: info
TYPE(MPI_Comm), INTENT(IN) :: comm
TYPE(C_PTR), INTENT(OUT) :: baseptr
TYPE(MPI_Win), INTENT(OUT) :: win
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
</pre>
</div>
</div>
 call presumably will
put the memory close to the
<i>socket</i>
 on which the process runs.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mpisharedwindow" aria-expanded="false" aria-controls="mpisharedwindow">
        C Code: mpisharedwindow
      </button>
    </h5>
  </div>
  <div id="mpisharedwindow" class="collapse">
  <pre>
// sharedbulk.c
MPI_Aint window_size; double *window_data; MPI_Win node_window;
if (onnode_procid==0)
  window_size = sizeof(double);
else window_size = 0;
MPI_Win_allocate_shared
  ( window_size,sizeof(double),MPI_INFO_NULL,
    nodecomm,
    &window_data,&node_window);
</pre>
</div>
</div>
<p name="switchToTextMode">

The memory allocated by 
<i>MPI_Win_allocate_shared</i>
 is
contiguous between the processes. This makes it possible to do address
calculation. However, if a cluster node has a 
<span title="acronym" ><i>NUMA</i></span>
 structure, for
instance if two sockets have memory directly attached to each, this
would increase latency for some processes. To prevent this, the key
<i>alloc_shared_noncontig</i>
 can be set to 
<tt>true</tt>
 in the
<i>MPI_Info</i>
 object.
<!-- environment: mpifour start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mpifour ]] -->
<remark>
<b>MPI 4 Standard only</b>
<p name="remark">
<!-- TranslatingLineGenerator mpifour ['mpifour'] -->
  In the contiguous case, the 
<i>mpi_minimum_memory_alignment</i>
  info argument
  (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-onesided.html#Windowcreationandallocation">9.1.1</a>
)
  applies only to the memory on the first process;
  in the non-contiguous case it applies to all.
</p name="remark">
<i>End of MPI 4 note</i>
</remark>
<!-- environment: mpifour end embedded generator -->
<p name="switchToTextMode">

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#winnoncontig" aria-expanded="false" aria-controls="winnoncontig">
        C Code: winnoncontig
      </button>
    </h5>
  </div>
  <div id="winnoncontig" class="collapse">
  <pre>
// numa.c
MPI_Info window_info;
MPI_Info_create(&window_info);
  MPI_Info_set(window_info,"alloc_shared_noncontig","true");
MPI_Win_allocate_shared( window_size,sizeof(double),window_info,
                         nodecomm,
                         &window_data,&node_window);
MPI_Info_free(&window_info);
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
Let's now consider a scenario where you spawn two MPI ranks per node,
and the node has 100G of memory.
Using the above option to allow for non-contiguous window allocation,
you hope that the windows of the two ranks are placed 50G apart.
However, if you print out the addresses, you will find that that they
are placed considerably closer together. For a small windows that distance
may be as little as&nbsp;4K, the size of a 
<i>small page</i>
.
</p>

<p name="switchToTextMode">
The reason for this mismatch is that an address that you obtain with
the ampersand operator in&nbsp;C is not a
<i>physical address</i>
, but a
<i>virtual address</i>
.
The translation of where pages are placed in physical memory
is determined by the 
<i>page table</i>
.
</p>

<p name="switchToTextMode">

<h3><a id="Queryingthesharedstructure">12.2.2</a> Querying the shared structure</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Queryingthesharedstructure">Querying the shared structure</a>
</p>
</p>

<p name="switchToTextMode">
Even though the window created above is shared, that doesn't mean it's
contiguous. Hence it is necessary to retrieve the pointer to the area
of each process that you want to communicate with:
<i>MPI_Win_shared_query</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Win_shared_query" aria-expanded="false" aria-controls="MPI_Win_shared_query">
        Routine reference: MPI_Win_shared_query
      </button>
    </h5>
  </div>
  <div id="MPI_Win_shared_query" class="collapse">
  <pre>
Semantics:
MPI_WIN_SHARED_QUERY(win, rank, size, disp_unit, baseptr)

Input arguments:
win:  shared memory window object (handle)
rank: rank in the group of window win (non-negative integer)
      or MPI_PROC_NULL

Output arguments:
size: size of the window segment (non-negative integer)
disp_unit: local unit size for displacements,
           in bytes (positive integer)
baseptr: address for load/store access to window segment (choice)

C:
int MPI_Win_shared_query
   (MPI_Win win, int rank, MPI_Aint *size, int *disp_unit,
    void *baseptr)

Fortran:
MPI_Win_shared_query(win, rank, size, disp_unit, baseptr, ierror)
USE, INTRINSIC :: ISO_C_BINDING, ONLY : C_PTR
TYPE(MPI_Win), INTENT(IN) :: win
INTEGER, INTENT(IN) :: rank
INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(OUT) :: size
INTEGER, INTENT(OUT) :: disp_unit
TYPE(C_PTR), INTENT(OUT) :: baseptr
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
</pre>
</div>
</div>
.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mpisharedpointer" aria-expanded="false" aria-controls="mpisharedpointer">
        C Code: mpisharedpointer
      </button>
    </h5>
  </div>
  <div id="mpisharedpointer" class="collapse">
  <pre>
MPI_Aint window_size0; int window_unit; double *win0_addr;
MPI_Win_shared_query( node_window,0,
			&window_size0,&window_unit, &win0_addr );
</pre>
</div>
</div>
<p name="switchToTextMode">

<h3><a id="Heatequationexample">12.2.3</a> Heat equation example</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Heatequationexample">Heat equation example</a>
</p>
</p>

<p name="switchToTextMode">
As an example, which consider the 1D heat equation. On each process we
create a local area of three point:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allocateshared3pt" aria-expanded="false" aria-controls="allocateshared3pt">
        C Code: allocateshared3pt
      </button>
    </h5>
  </div>
  <div id="allocateshared3pt" class="collapse">
  <pre>
// sharedshared.c
MPI_Win_allocate_shared(3,sizeof(int),info,sharedcomm,&shared_baseptr,&shared_window);
</pre>
</div>
</div>
</p>

<h3><a id="Sharedbulkdata">12.2.4</a> Shared bulk data</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Sharedbulkdata">Shared bulk data</a>
</p>
<p name="switchToTextMode">

In applications such as 
<i>ray tracing</i>
, there is a read-only
large data object (the objects in the scene to be rendered) that is
needed by all processes. In traditional MPI, this would need to be
stored redundantly on each process, which leads to large memory
demands. With MPI shared memory we can store the data object once per
node. Using as above 
<i>MPI_Comm_split_type</i>
 to find a
communicator per 
<span title="acronym" ><i>NUMA</i></span>
 domain, we store the object on process zero
of this node communicator.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Let the `shared' data originate on process zero in
<i>MPI_COMM_WORLD</i>
. Then:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
create a communicator per shared memory domain;
<li>
create a communicator for all the processes with number zero on their
    node;
<li>
broadcast the shared data to the processes zero on each node.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<br>
(See source <tt>shareddata</tt>)
</exercise>
<!-- environment: exercise end embedded generator -->
</div>
<a href="index.html">Back to Table of Contents</a>
