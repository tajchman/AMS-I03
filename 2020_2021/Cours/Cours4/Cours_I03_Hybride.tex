\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}

\urlstyle{same}

\usepackage{xcolor}

\lstdefinestyle{base}{
	language=C++,
	emptylines=1,
	breaklines=true,
	basicstyle=\ttfamily\color{black},
	moredelim=**[is][\bf\color{red}]{@}{@},
}

\usetheme[]{boxes}
\usecolortheme{seagull}
\addtobeamertemplate{navigation symbols}{}{%
	\usebeamerfont{footline}%
	\usebeamercolor[fg]{footline}%
	\hspace{2em}%
	\insertframenumber/\inserttotalframenumber
}

%\usepackage{french}
\title{Programmation parallèle hybride}
\author{Marc Tajchman}\institute{CEA - DEN/DM2S/STMF/LMES}
\date{10/12/2020}

\begin{document}
\begin{frame}
	\titlepage
\end{frame}

\large
\begin{frame}
	\frametitle{Cas envisagés}

Du fait qu'il y a souvent plusieurs dispositifs matériels (clusters, machines multi-c\oe urs, GPU, etc.) et plusieurs outils logiciels associés, il est intéressant d'essayer de combiner leur utilisation.

\vfill
En fonction de ce qui est disponible sur les machines
\begin{enumerate}
	\item clusters (plus généralement, machines parallèles multi-n\oe uds) : MPI,
	\item processeurs multi-c\oe urs : OpenMP (et autres outils : TBB, std::threads, ...),
	\item accélérateurs de calcul (GPU ou autres) : Cuda, OpenCL,
	\item outils PGAS (``partitionned global addressing system") : pour information (faibles performances),
\end{enumerate}

\vfill
on testera plusieurs combinaisons.
\end{frame}

\begin{frame}
\frametitle{Machines multi-n\oe uds et multi-c\oe urs}

Pour utiliser la puissance de calcul de ce type de machine, on a souvent le choix entre 2 possibilités :
\medskip
\begin{itemize}
	\item {\bf MPI} pour le parallélisme multi-n\oe uds et {\bf MPI} pour le parallélisme multi-c\oe urs (plusieurs processus MPI sur chaque n\oe ud).
\medskip
	\item {\bf MPI} pour le parallélisme multi-n\oe uds et {\bf OpenMP} pour le parallélisme multi-c\oe urs (un processus MPI sur chaque n\oe ud).
\end{itemize}
\vfill

Sur des simulations de taille petite ou moyenne, on préfère souvent le premier choix pour des raisons de simplicité. 

Par contre, sur des simulations de (très) grande taille, le ``tout MPI'' atteint plus rapidement ses limites d'utilisation.
\end{frame}

\begin{frame}
	
\vfill

Avantages/inconvénients du tout MPI:
	
\vfill
\begin{itemize}
	\item[\LARGE $\mathbf\oplus$] programmation MPI n\oe uds-c\oe urs identique à la programmation MPI entre les n\oe uds
	\vfill
	
	\item[\LARGE $\mathbf\oplus$] les librairies MPI sont de mieux en mieux optimisées (en particulier si plieurs processus sont sur le même processeur)
	
\vfill
	\item[\LARGE $\mathbf\ominus$] le nombre de processus MPI augmente plus vite (n° c\oe urs $\times$ n° n\oe uds), les structures internes de MPI, la mémoire utilisée pour les communications aussi (``cellules fantomes'', ``halos'')
	\bigskip
	
\bigskip

	\begin{quote}
On atteint actuellement les limites des machines parallèles sur des simulations avec des nombres de processus $> 10^6$.
	\end{quote}
	
\end{itemize}
\vfill
\end{frame}

\begin{frame}


\vfill
	Avantages/inconvénients de la combinaison MPI-OpenMP:
	
\vfill
	\begin{itemize}
		\item[\LARGE $\mathbf\oplus$] les possibilités en nombre de n\oe uds-c\oe urs sont plus importantes
		
\begin{quotation}\noindent%
Le nombre total de processus MPI diminue (un seul processus MPI par n\oe ud de calcul.)
\end{quotation}
	
\vfill
		\item[\LARGE $\mathbf\ominus$] la programmation MPI sur les n\oe uds et OpenMP sur les c\oe urs est plus complexe
	
\vfill
		\item[\LARGE $\mathbf\ominus$] l'amélioration des performances n'est pas toujours évidente surtout pour des nombres de processus petits ou moyens.
	\end{itemize}
	
	\vfill
\end{frame}

\begin{frame}
	Exemples4/MemoireMPI : ressource mémoire utilisée par MPI (MPI\_Init et MPI\_Finalize, seulement, pas d'échange de messages).
	\bigskip
	
	Lire le fichier README.txt pour des instructions
\end{frame}

\begin{frame}
	Combinaison des modèles MPI et OpenMP en modèle hybride MPI-OpenMP:

\begin{center}
	\includegraphics[scale=0.45]{../../Images/modeleHybride}
\end{center}

\begin{itemize}
	\item Les différents processus $P_0$, $P_1$, ... ont chacun leur mémoire locale $M_0$, $M_1$, ....
	\item A l'intérieur de chaque processus $P_i$, un ou plusieurs threads travaillent avec la mémoire locale $M_i$.
\end{itemize}
\end{frame}

\begin{frame}
	\vfill
\begin{itemize}
	\item Pas de synchronisation a priori entre les sections multi-threads entre les différents processus, donc besoin éventuel de combiner des barrières OpenMP et MPI
	\vfill
	
	\item 2 types de réductions dans OpenMP et MPI, donc pour faire une réduction complète, il faut combiner ces 2 réductions
	\vfill
	
	\item On peut a priori échanger des messages entre chaque threads de 2 processus différents
\end{itemize}
	\vfill	
	
\end{frame}

\begin{frame}
	Exemple sur 2 processus, chacun avec plusieurs threads
	\includegraphics[scale=0.4]{../../Images/enchainementHybride1}
\end{frame}

\begin{frame}
	Quelques types différents de messages
	\includegraphics[scale=0.4]{../../Images/enchainementHybride2}
\end{frame}

\begin{frame}
\vfill
Le grand nombre de situations possibles est très difficile à gérer pour ceux qui développent les libraires OpenMP et MPI

	\vfill
Pour cette raison, 4 niveaux de compatibilité entre OpenMP et MPI, ont été définis:
	\vfill

\begin{itemize}
	\item {\tt\textcolor{blue}{MPI\_THREAD\_SINGLE}}:
	
	Un seul thread par processus (donc pas d'OpenMP)
	\item {\tt\textcolor{blue}{MPI\_THREAD\_FUNNELED}}:
	
	Parmi les threads, seul celui qui a initialisé MPI peut utiliser des fonctions MPI
	\item {\tt\textcolor{blue}{MPI\_THREAD\_SERIALIZED}}:
	
	A un instant donné, un seul thread peut utiliser des fonctions MPI (mais pas toujours le même thread)
	\item {\tt\textcolor{blue}{MPI\_THREAD\_MULTIPLE}}:
	
	Pas de restrictions, tous les threads peuvent utiliser les fonctions MPI.
	
\end{itemize}

\vfill

\end{frame}

\begin{frame}[fragile]
Plus le niveau de compatibilité permet de souplesse dans l'utilisation des fonctions MPI et des pragmas OpenMP, plus la gestion interne des threads dans les processus est coûteuse.

\textcolor{blue}{Donc, il faut choisir le niveau le plus bas suffisant pour l'algorithme du code}

\vfill
Pour choisir le niveau de compatibilité, il faut remplacer
\begin{lstlisting}
MPI_Init(int *argc, char ***argv)
\end{lstlisting}

par
\begin{lstlisting}
MPI_Init_thread(int *argc, char ***argv,
                int required, int *provided)
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]
	où {\tt required} est un entier qui peut prendre une des 4 valeurs \begin{itemize}
		\item {\tt MPI\_THREAD\_SINGLE},
		\item {\tt MPI\_THREAD\_FUNNELED},
		\item {\tt MPI\_THREAD\_SERIALIZED},
		\item {\tt MPI\_THREAD\_MULTIPLE}
	\end{itemize}   
	\vfill
	
	et {\tt provided} est un entier qui reçoit le niveau de compatibilité effectivement proposé par la version de MPI
	
	\vfill
	\textcolor{red}{Il est important de comparer les valeurs de {\tt required} et {\tt provided} pour vérifier que le niveau demandé est effectivement disponible.} 
\end{frame}

\begin{frame}
	Exemples4/Hybrides : différents niveaux de combinaison MPI-OpenMP
\end{frame}
\end{document}
